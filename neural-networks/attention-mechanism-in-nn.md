# Attention Mechanism in NN

## Idea

For each input that LSTM \(Encoder\) reads, the attention-mechanism takes into account several other inputs at the same time and decides which ones are important by attributing different weights to those inputs.

## Multi-Head Attention

### Scaled Dot-Product Attention

![Scaled Dot-Product Attention](../.gitbook/assets/image%20%281%29.png)



## References

1. [Medium: Transformer](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)

